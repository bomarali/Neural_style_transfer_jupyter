{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take two of nst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.io\n",
    "import errno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare arguments into a dictionary\n",
    "inputs = {\n",
    "    'content_image' : os.path.join('.\\image_input', 'not_id.jpg'),\n",
    "    # string - path to content image\n",
    "    'content_weight' : 100,\n",
    "    # float - alpha for total loss function\n",
    "    'content_layers' : ['conv4_2'],\n",
    "    # list of strings - list of layers used for content loss\n",
    "    'content_layers_weights' : [1.0],\n",
    "    # list of floats - content layers weights for content loss\n",
    "    'content_loss_function' : 1,\n",
    "    # integer - 1 or 2 or 3\n",
    "    'style_images' : [os.path.join('.\\styles', '1.jpg')] ,\n",
    "    # list of string - list of pathes to style images\n",
    "    'style_images_weights' : [1.0],\n",
    "    # list of floats - list of style images' weights\n",
    "    'style_weight' : 10000,\n",
    "    # float - beta for total loss function\n",
    "    'style_layers' : ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1'],\n",
    "    # list of strings - layers used for style loss\n",
    "    'style_layers_weights' : [0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "    # list of floats - list of style layers weights\n",
    "    'total_variation_weight' : .001,\n",
    "    # float - total variation weight for the total loss function\n",
    "    'image_max_size' : 700,\n",
    "    # int - maximum dimension (width or height) of images in question\n",
    "    'verbose' : True,\n",
    "    # boolean - print a lot or not\n",
    "    'generated_image_initialization' : 'random',\n",
    "    # string from 'content', 'style', 'random'\n",
    "    'noise_ratio' : 0.3, \n",
    "    # float from 0.0 to 1.0 - interpolation ratio from content image to a full noise image \n",
    "    'model_weights' : 'imagenet-vgg-verydeep-19.mat',\n",
    "    # string - path to pre-trained model weights (.mat file)\n",
    "    'pooling_type' : 'avg',\n",
    "    # string either 'avg' or 'max'\n",
    "    'device' : '/gpu:0',\n",
    "    # string - device used to run tensorflow '/cpu:0' or '/gpu:0' or else\n",
    "    'style_mask' : False, \n",
    "    # boolean - use mask \n",
    "    'style_masks_images' : [os.path.join('.\\image_input', 'face_mask_inv.png')], \n",
    "    # list of strings - list of paths to style mask images\n",
    "    # !NB number of style_mask_images should be equal to number of styles\n",
    "    'original_colors' : False,\n",
    "    # boolean - use original content colors\n",
    "    'color_convert_type' : 'yuv',\n",
    "    # string - one of ['yuv', 'ycrcb', 'luv', 'lab']\n",
    "    'optimizer_function' : 'adam',\n",
    "    # string - either lfbgs or adam\n",
    "    'learning_rate' : 3.0,\n",
    "    # float - learning rate for optimization\n",
    "    'max_iterations' : 500,\n",
    "    # integer - number of max iterations done in image generation\n",
    "}\n",
    "\n",
    "def normalize(weights):\n",
    "    denom = sum(weights)\n",
    "    if denom > 0.:\n",
    "        return [float(i) / denom for i in weights]\n",
    "    else: return [0.] * len(weights)\n",
    "    \n",
    "inputs['style_layers_weights'] = normalize(inputs['style_layers_weights'])\n",
    "inputs['content_layers_weights'] = normalize(inputs['content_layers_weights'])\n",
    "inputs['style_images_weights'] = normalize(inputs['style_images_weights'])\n",
    "\n",
    "def check_image(img):\n",
    "    if cv2.imread(img, cv2.IMREAD_GRAYSCALE) is None:\n",
    "        raise OSError(errno.ENOENT, \"No such file\", img)\n",
    "        \n",
    "check_image(inputs['content_image'])\n",
    "for style_image in inputs['style_images']:\n",
    "    check_image(style_image)\n",
    "for style_mask_image in inputs['style_masks_images']:\n",
    "    check_image(style_mask_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_image(content_image, image_max_size):\n",
    "    img = cv2.imread(content_image, cv2.IMREAD_COLOR)\n",
    "    img = img.astype(np.float32)\n",
    "    h, w, d = img.shape\n",
    "    # resize if > max size\n",
    "    if h > w and h > image_max_size:\n",
    "        w = (float(image_max_size) / float(h)) * w\n",
    "        img = cv2.resize(img, dsize=(int(w), image_max_size), interpolation=cv2.INTER_AREA)\n",
    "    if w > image_max_size:\n",
    "        h = (float(image_max_size) / float(w)) * h\n",
    "        img = cv2.resize(img, dsize=(image_max_size, int(h)), interpolation=cv2.INTER_AREA)\n",
    "    img = preprocess(img)\n",
    "    return img\n",
    "\n",
    "def preprocess(img):\n",
    "    # bgr to rgb\n",
    "    img = img[...,::-1]\n",
    "    # shape (h, w, d) to (1, h, w, d)\n",
    "    img = img[np.newaxis,:,:,:]\n",
    "    img -= np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3))\n",
    "    return img\n",
    "\n",
    "def get_style_images(style_images, content_image):\n",
    "    _, ch, cw, cd = content_image.shape\n",
    "    style_imgs = []\n",
    "    for style_fn in style_images:\n",
    "        # bgr image\n",
    "        img = cv2.imread(style_fn, cv2.IMREAD_COLOR)\n",
    "        img = img.astype(np.float32)\n",
    "        img = cv2.resize(img, dsize=(cw, ch), interpolation=cv2.INTER_AREA)\n",
    "        img = preprocess(img)\n",
    "        style_imgs.append(img)\n",
    "    return style_imgs\n",
    "\n",
    "def get_initial_image(generated_image_initialization, content_image, style_images, noise_ratio, frame=None):\n",
    "    if generated_image_initialization == 'content':\n",
    "        return content_image\n",
    "    elif generated_image_initialization == 'style':\n",
    "        return style_images[0]\n",
    "    elif generated_image_initialization == 'random':\n",
    "        noise_image = np.random.uniform(-20., 20., content_image.shape).astype(np.float32)\n",
    "        initial_image = noise_ratio * noise_image + (1.-noise_ratio) * content_image\n",
    "        return initial_image\n",
    "  # only for video frames\n",
    "    #elif init_type == 'prev':\n",
    "        #init_img = get_prev_frame(frame)\n",
    "        #return init_img\n",
    "    #elif init_type == 'prev_warped':\n",
    "        #init_img = get_prev_warped_frame(frame)\n",
    "        #return init_img\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug_piece\n",
    "#print(inputs['content_image'])\n",
    "#for style_image in inputs['style_images']:\n",
    "    #print(style_image)\n",
    "#for mask_image in inputs['style_masks_images']:\n",
    "    #print(mask_image)\n",
    "#content_image = get_content_image(inputs['content_image'], inputs['image_max_size'])\n",
    "#style_images = get_style_images(inputs['style_images'], content_image)\n",
    "#initial_image = get_initial_image(inputs['generated_image_initialization'], content_image, style_images, inputs['noise_ratio'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(layer_name, layer_input, W, verbose = True):\n",
    "    conv = tf.nn.conv2d(layer_input, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    if verbose: print('--{} | shape={} | weights_shape={}'.format(layer_name, \n",
    "        conv.get_shape(), W.get_shape()))\n",
    "    return conv\n",
    "\n",
    "def relu_layer(layer_name, layer_input, b, verbose = True):\n",
    "    relu = tf.nn.relu(layer_input + b)\n",
    "    if verbose: \n",
    "        print('--{} | shape={} | bias_shape={}'.format(layer_name, relu.get_shape(), b.get_shape()))\n",
    "    return relu\n",
    "\n",
    "def pool_layer(layer_name, layer_input, verbose = True, pooling_type = 'avg'):\n",
    "    if pooling_type == 'avg':\n",
    "        pool = tf.nn.avg_pool(layer_input, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    elif pooling_type == 'max':\n",
    "        pool = tf.nn.max_pool(layer_input, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    if verbose: \n",
    "        print('--{}   | shape={}'.format(layer_name, pool.get_shape()))\n",
    "    return pool\n",
    "\n",
    "def get_weights(vgg_layers, i):\n",
    "    weights = vgg_layers[i][0][0][2][0][0]\n",
    "    W = tf.constant(weights)\n",
    "    return W\n",
    "\n",
    "def get_bias(vgg_layers, i):\n",
    "    bias = vgg_layers[i][0][0][2][0][1]\n",
    "    b = tf.constant(np.reshape(bias, (bias.size)))\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_image, model_weights, pooling_type, verbose = True):\n",
    "    if verbose: print('\\nBUILDING VGG-19 NETWORK')\n",
    "    net = {}\n",
    "    _, h, w, d     = input_image.shape\n",
    "  \n",
    "    if verbose: print('loading model weights...')\n",
    "    vgg_rawnet     = scipy.io.loadmat(model_weights)\n",
    "    vgg_layers     = vgg_rawnet['layers'][0]\n",
    "    if verbose: print('constructing layers...')\n",
    "    net['input']   = tf.Variable(np.zeros((1, h, w, d), dtype=np.float32))\n",
    "\n",
    "    if verbose: print('LAYER GROUP 1')\n",
    "    net['conv1_1'] = conv_layer('conv1_1', net['input'], W=get_weights(vgg_layers, 0))\n",
    "    net['relu1_1'] = relu_layer('relu1_1', net['conv1_1'], b=get_bias(vgg_layers, 0))\n",
    "    net['conv1_2'] = conv_layer('conv1_2', net['relu1_1'], W=get_weights(vgg_layers, 2))\n",
    "    net['relu1_2'] = relu_layer('relu1_2', net['conv1_2'], b=get_bias(vgg_layers, 2))\n",
    "    net['pool1']   = pool_layer('pool1', net['relu1_2'], pooling_type)\n",
    "\n",
    "    if verbose: print('LAYER GROUP 2')  \n",
    "    net['conv2_1'] = conv_layer('conv2_1', net['pool1'], W=get_weights(vgg_layers, 5))\n",
    "    net['relu2_1'] = relu_layer('relu2_1', net['conv2_1'], b=get_bias(vgg_layers, 5))\n",
    "    net['conv2_2'] = conv_layer('conv2_2', net['relu2_1'], W=get_weights(vgg_layers, 7))\n",
    "    net['relu2_2'] = relu_layer('relu2_2', net['conv2_2'], b=get_bias(vgg_layers, 7))\n",
    "    net['pool2']   = pool_layer('pool2', net['relu2_2'], pooling_type)\n",
    "  \n",
    "    if verbose: print('LAYER GROUP 3')\n",
    "    net['conv3_1'] = conv_layer('conv3_1', net['pool2'], W=get_weights(vgg_layers, 10))\n",
    "    net['relu3_1'] = relu_layer('relu3_1', net['conv3_1'], b=get_bias(vgg_layers, 10))\n",
    "    net['conv3_2'] = conv_layer('conv3_2', net['relu3_1'], W=get_weights(vgg_layers, 12))\n",
    "    net['relu3_2'] = relu_layer('relu3_2', net['conv3_2'], b=get_bias(vgg_layers, 12))\n",
    "    net['conv3_3'] = conv_layer('conv3_3', net['relu3_2'], W=get_weights(vgg_layers, 14))\n",
    "    net['relu3_3'] = relu_layer('relu3_3', net['conv3_3'], b=get_bias(vgg_layers, 14))\n",
    "    net['conv3_4'] = conv_layer('conv3_4', net['relu3_3'], W=get_weights(vgg_layers, 16))\n",
    "    net['relu3_4'] = relu_layer('relu3_4', net['conv3_4'], b=get_bias(vgg_layers, 16))\n",
    "    net['pool3']   = pool_layer('pool3', net['relu3_4'], pooling_type)\n",
    "\n",
    "    if verbose: print('LAYER GROUP 4')\n",
    "    net['conv4_1'] = conv_layer('conv4_1', net['pool3'], W=get_weights(vgg_layers, 19))\n",
    "    net['relu4_1'] = relu_layer('relu4_1', net['conv4_1'], b=get_bias(vgg_layers, 19))\n",
    "    net['conv4_2'] = conv_layer('conv4_2', net['relu4_1'], W=get_weights(vgg_layers, 21))\n",
    "    net['relu4_2'] = relu_layer('relu4_2', net['conv4_2'], b=get_bias(vgg_layers, 21))\n",
    "    net['conv4_3'] = conv_layer('conv4_3', net['relu4_2'], W=get_weights(vgg_layers, 23))\n",
    "    net['relu4_3'] = relu_layer('relu4_3', net['conv4_3'], b=get_bias(vgg_layers, 23))\n",
    "    net['conv4_4'] = conv_layer('conv4_4', net['relu4_3'], W=get_weights(vgg_layers, 25))\n",
    "    net['relu4_4'] = relu_layer('relu4_4', net['conv4_4'], b=get_bias(vgg_layers, 25))\n",
    "    net['pool4']   = pool_layer('pool4', net['relu4_4'], pooling_type)\n",
    "\n",
    "    if verbose: print('LAYER GROUP 5')\n",
    "    net['conv5_1'] = conv_layer('conv5_1', net['pool4'], W=get_weights(vgg_layers, 28))\n",
    "    net['relu5_1'] = relu_layer('relu5_1', net['conv5_1'], b=get_bias(vgg_layers, 28))\n",
    "    net['conv5_2'] = conv_layer('conv5_2', net['relu5_1'], W=get_weights(vgg_layers, 30))\n",
    "    net['relu5_2'] = relu_layer('relu5_2', net['conv5_2'], b=get_bias(vgg_layers, 30))\n",
    "    net['conv5_3'] = conv_layer('conv5_3', net['relu5_2'], W=get_weights(vgg_layers, 32))\n",
    "    net['relu5_3'] = relu_layer('relu5_3', net['conv5_3'], b=get_bias(vgg_layers, 32))\n",
    "    net['conv5_4'] = conv_layer('conv5_4', net['relu5_3'], W=get_weights(vgg_layers, 34))\n",
    "    net['relu5_4'] = relu_layer('relu5_4', net['conv5_4'], b=get_bias(vgg_layers, 34))\n",
    "    net['pool5']   = pool_layer('pool5', net['relu5_4'], pooling_type)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_image(mask_img, width, height):\n",
    "    img = cv2.imread(mask_img, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, dsize=(width, height), interpolation=cv2.INTER_AREA)\n",
    "    img = img.astype(np.float32)\n",
    "    mx = np.amax(img)\n",
    "    img /= mx\n",
    "    return img\n",
    "\n",
    "def mask_style_layer(a, x, mask_img):\n",
    "    _, h, w, d = a.get_shape()\n",
    "    mask = get_mask_image(mask_img, w.value, h.value)\n",
    "    mask = tf.convert_to_tensor(mask)\n",
    "    tensors = []\n",
    "    for _ in range(d.value): \n",
    "        tensors.append(mask)\n",
    "    mask = tf.stack(tensors, axis=2)\n",
    "    #mask = tf.stack(mask, axis=0) \n",
    "    mask = tf.expand_dims(mask, 0)\n",
    "    a = tf.multiply(a, mask)\n",
    "    x = tf.multiply(x, mask)\n",
    "    return a, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_layer_loss(a, x):\n",
    "    _, h, w, d = a.get_shape()\n",
    "    M = h.value * w.value\n",
    "    N = d.value\n",
    "    A = gram_matrix(a, M, N)\n",
    "    G = gram_matrix(x, M, N)\n",
    "    loss = (1./(4 * N**2 * M**2)) * tf.reduce_sum(tf.pow((G - A), 2))\n",
    "    return loss\n",
    "\n",
    "def gram_matrix(x, area, depth):\n",
    "    F = tf.reshape(x, (area, depth))\n",
    "    G = tf.matmul(tf.transpose(F), F)\n",
    "    return G\n",
    "\n",
    "def content_layer_loss(p, x, content_loss_function):\n",
    "    _, h, w, d = p.get_shape()\n",
    "    M = h.value * w.value\n",
    "    N = d.value\n",
    "    if content_loss_function   == 1:\n",
    "        K = 1. / (2. * N**0.5 * M**0.5)\n",
    "    elif args.content_loss_function == 2:\n",
    "        K = 1. / (N * M)\n",
    "    elif args.content_loss_function == 3:  \n",
    "        K = 1. / 2.\n",
    "    loss = K * tf.reduce_sum(tf.pow((x - p), 2))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_masked_style_losses(sess, net, style_images, style_images_weights, style_masks_images, style_layers, style_layers_weights):\n",
    "    total_style_loss = 0.0\n",
    "    weights = style_images_weights\n",
    "    masks = style_masks_images\n",
    "    for img, img_weight, img_mask in zip(style_images, weights, masks):\n",
    "        sess.run(net['input'].assign(img))\n",
    "        style_loss = 0.0\n",
    "        for layer, weight in zip(style_layers, style_layers_weights):\n",
    "            a = sess.run(net[layer])\n",
    "            x = net[layer]\n",
    "            a = tf.convert_to_tensor(a)\n",
    "            a, x = mask_style_layer(a, x, img_mask)\n",
    "            style_loss += style_layer_loss(a, x) * weight\n",
    "        style_loss /= float(len(style_layers))\n",
    "        total_style_loss += (style_loss * img_weight)\n",
    "    total_style_loss /= float(len(style_images))\n",
    "    return total_style_loss\n",
    "\n",
    "def sum_style_losses(sess, net, style_images, style_images_weights, style_layers, style_layers_weights):\n",
    "    total_style_loss = 0.0\n",
    "    weights = style_images_weights\n",
    "    for img, img_weight in zip(style_images, weights):\n",
    "        # for each style image and the corresponding weight runs the net and caclulates activations\n",
    "        sess.run(net['input'].assign(img))\n",
    "        style_loss = 0.0\n",
    "        for layer, weight in zip(style_layers, style_layers_weights):\n",
    "            # for each style layer and the corresponding weight gets the activation\n",
    "            a = sess.run(net[layer])\n",
    "            x = net[layer]\n",
    "            # x is the variable activation of the net at layer given generated image\n",
    "            a = tf.convert_to_tensor(a)\n",
    "            style_loss += style_layer_loss(a, x) * weight\n",
    "        style_loss /= float(len(style_layers))\n",
    "        total_style_loss += (style_loss * img_weight)\n",
    "    total_style_loss /= float(len(style_images))\n",
    "    return total_style_loss\n",
    "\n",
    "def sum_content_losses(sess, net, content_image, content_layers, content_layers_weights, content_loss_function):\n",
    "    sess.run(net['input'].assign(content_image))\n",
    "    content_loss = 0.0\n",
    "    for layer, weight in zip(content_layers, content_layers_weights):\n",
    "        p = sess.run(net[layer])\n",
    "        x = net[layer]\n",
    "        p = tf.convert_to_tensor(p)\n",
    "        content_loss += content_layer_loss(p, x, content_loss_function) * weight\n",
    "    content_loss /= float(len(content_layers))\n",
    "    return content_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = build_model(content_image, inputs['model_weights'], inputs['pooling_type'], inputs['verbose'])\n",
    "#with tf.device(inputs['device']), tf.Session() as sess:\n",
    "    #L_style = sum_masked_style_losses(sess, model, style_images, inputs['style_images_weights'],\n",
    "                                     #inputs['style_masks_images'], inputs['style_layers'], inputs['style_layers_weights'])\n",
    "    #L_style = sum_style_losses(sess, model, style_images,\n",
    "                               #inputs['style_images_weights'], inputs['style_layers'], inputs['style_layers_weights'])\n",
    "    #L_content = sum_content_losses(sess, model, content_image, inputs['content_layers'], inputs['content_layers_weights'],\n",
    "                                  #inputs['content_loss_function'])\n",
    "    #L_tv = total_variation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_original_colors(content_image, stylized_image, color_convert_type):\n",
    "    content_image  = postprocess(content_image)\n",
    "    stylized_image = postprocess(stylized_image)\n",
    "    if color_convert_type == 'yuv':\n",
    "        cvt_type = cv2.COLOR_BGR2YUV\n",
    "        inv_cvt_type = cv2.COLOR_YUV2BGR\n",
    "    elif color_convert_type == 'ycrcb':\n",
    "        cvt_type = cv2.COLOR_BGR2YCR_CB\n",
    "        inv_cvt_type = cv2.COLOR_YCR_CB2BGR\n",
    "    elif color_convert_type == 'luv':\n",
    "        cvt_type = cv2.COLOR_BGR2LUV\n",
    "        inv_cvt_type = cv2.COLOR_LUV2BGR\n",
    "    elif color_convert_type == 'lab':\n",
    "        cvt_type = cv2.COLOR_BGR2LAB\n",
    "        inv_cvt_type = cv2.COLOR_LAB2BGR\n",
    "    content_cvt = cv2.cvtColor(content_image, cvt_type)\n",
    "    stylized_cvt = cv2.cvtColor(stylized_image, cvt_type)\n",
    "    c1, _, _ = cv2.split(stylized_cvt)\n",
    "    _, c2, c3 = cv2.split(content_cvt)\n",
    "    merged = cv2.merge((c1, c2, c3))\n",
    "    dst = cv2.cvtColor(merged, inv_cvt_type).astype(np.float32)\n",
    "    dst = preprocess(dst)\n",
    "    return dst\n",
    "\n",
    "def postprocess(img):\n",
    "    img += np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3))\n",
    "    # shape (1, h, w, d) to (h, w, d)\n",
    "    img = img[0]\n",
    "    img = np.clip(img, 0, 255).astype('uint8')\n",
    "    # rgb to bgr\n",
    "    img = img[...,::-1]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_image(path, img):\n",
    "    img = postprocess(img)\n",
    "    cv2.imwrite(path, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(L_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stylize(content_image, style_images, initial_image, inputs, frame = None):\n",
    "    with tf.device(inputs['device']), tf.Session() as sess:\n",
    "        # setup network\n",
    "        net = build_model(content_image, inputs['model_weights'], inputs['pooling_type'], inputs['verbose'])\n",
    "\n",
    "        # style loss\n",
    "        if inputs['style_mask']:\n",
    "            L_style = sum_masked_style_losses(sess, net, style_images, inputs['style_images_weights'],\n",
    "                                              inputs['style_masks_images'], inputs['style_layers'],\n",
    "                                              inputs['style_layers_weights'])\n",
    "        else:\n",
    "            L_style = sum_style_losses(sess, net, style_images, inputs['style_images_weights'],\n",
    "                                       inputs['style_layers'], inputs['style_layers_weights'])\n",
    "\n",
    "        # content loss\n",
    "        L_content = sum_content_losses(sess, net, content_image, inputs['content_layers'],\n",
    "                                       inputs['content_layers_weights'], inputs['content_loss_function'])\n",
    "\n",
    "        # denoising loss\n",
    "        L_tv = tf.image.total_variation(net['input'])\n",
    "\n",
    "        # total loss\n",
    "        L_total  = inputs['content_weight'] * L_content\n",
    "        L_total += inputs['style_weight']  * L_style\n",
    "        L_total += inputs['total_variation_weight'] * L_tv\n",
    "\n",
    "        # video temporal loss\n",
    "        #if args.video and frame > 1:\n",
    "          #gamma      = args.temporal_weight\n",
    "          #L_temporal = sum_shortterm_temporal_losses(sess, net, frame, init_img)\n",
    "          #L_total   += gamma * L_temporal\n",
    "\n",
    "        # optimization algorithm\n",
    "        if inputs['optimizer_function'] == 'lbfgs':\n",
    "            optimizer = tf.contrib.opt.ScipyOptimizerInterface(L_total, method='L-BFGS-B')\n",
    "            if inputs['verbose']: print('\\nMINIMIZING LOSS USING: L-BFGS OPTIMIZER')\n",
    "            init_op = tf.global_variables_initializer()\n",
    "            sess.run(init_op)\n",
    "            sess.run(net['input'].assign(initial_image))\n",
    "            optimizer.minimize(sess)\n",
    "        elif inputs['optimizer_function'] == 'adam':\n",
    "            optimizer = tf.train.AdamOptimizer(inputs['learning_rate'])\n",
    "            if inputs['verbose']: print('\\nMINIMIZING LOSS USING: ADAM OPTIMIZER')\n",
    "            train_op = optimizer.minimize(L_total)\n",
    "            init_op = tf.global_variables_initializer()\n",
    "            sess.run(init_op)\n",
    "            sess.run(net['input'].assign(initial_image))\n",
    "            iterations = 0\n",
    "            while (iterations < inputs['max_iterations']):\n",
    "                sess.run(train_op)\n",
    "                if iterations % 20 == 0 and inputs['verbose']:\n",
    "                    curr_loss = L_total.eval()\n",
    "                    print(\"At iterate {}\\tf=  {}\".format(iterations, curr_loss))\n",
    "                iterations += 1\n",
    "\n",
    "        output_image = sess.run(net['input'])\n",
    "\n",
    "        if inputs['original_colors']:\n",
    "            output_image = convert_to_original_colors(np.copy(content_image), output_image, inputs['color_convert_type'])\n",
    "\n",
    "        #if args.video:\n",
    "          #write_video_output(frame, output_img)\n",
    "        #else:\n",
    "        write_image('generated_image.png', output_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_single_image(inputs):\n",
    "    content_image = get_content_image(inputs['content_image'], inputs['image_max_size'])\n",
    "    style_images = get_style_images(inputs['style_images'], content_image)\n",
    "    with tf.Graph().as_default():\n",
    "        print('\\n---- RENDERING SINGLE IMAGE ----\\n')\n",
    "        initial_image = get_initial_image(inputs['generated_image_initialization'],\n",
    "                                          content_image, style_images, inputs['noise_ratio'])\n",
    "        tick = time.time()\n",
    "        stylize(content_image, style_images, initial_image, inputs)\n",
    "        tock = time.time()\n",
    "        print('Single image elapsed time: {}'.format(tock - tick))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- RENDERING SINGLE IMAGE ----\n",
      "\n",
      "\n",
      "BUILDING VGG-19 NETWORK\n",
      "loading model weights...\n",
      "constructing layers...\n",
      "LAYER GROUP 1\n",
      "--conv1_1 | shape=(1, 700, 525, 64) | weights_shape=(3, 3, 3, 64)\n",
      "--relu1_1 | shape=(1, 700, 525, 64) | bias_shape=(64,)\n",
      "--conv1_2 | shape=(1, 700, 525, 64) | weights_shape=(3, 3, 64, 64)\n",
      "--relu1_2 | shape=(1, 700, 525, 64) | bias_shape=(64,)\n",
      "--pool1   | shape=(1, 350, 263, 64)\n",
      "LAYER GROUP 2\n",
      "--conv2_1 | shape=(1, 350, 263, 128) | weights_shape=(3, 3, 64, 128)\n",
      "--relu2_1 | shape=(1, 350, 263, 128) | bias_shape=(128,)\n",
      "--conv2_2 | shape=(1, 350, 263, 128) | weights_shape=(3, 3, 128, 128)\n",
      "--relu2_2 | shape=(1, 350, 263, 128) | bias_shape=(128,)\n",
      "--pool2   | shape=(1, 175, 132, 128)\n",
      "LAYER GROUP 3\n",
      "--conv3_1 | shape=(1, 175, 132, 256) | weights_shape=(3, 3, 128, 256)\n",
      "--relu3_1 | shape=(1, 175, 132, 256) | bias_shape=(256,)\n",
      "--conv3_2 | shape=(1, 175, 132, 256) | weights_shape=(3, 3, 256, 256)\n",
      "--relu3_2 | shape=(1, 175, 132, 256) | bias_shape=(256,)\n",
      "--conv3_3 | shape=(1, 175, 132, 256) | weights_shape=(3, 3, 256, 256)\n",
      "--relu3_3 | shape=(1, 175, 132, 256) | bias_shape=(256,)\n",
      "--conv3_4 | shape=(1, 175, 132, 256) | weights_shape=(3, 3, 256, 256)\n",
      "--relu3_4 | shape=(1, 175, 132, 256) | bias_shape=(256,)\n",
      "--pool3   | shape=(1, 88, 66, 256)\n",
      "LAYER GROUP 4\n",
      "--conv4_1 | shape=(1, 88, 66, 512) | weights_shape=(3, 3, 256, 512)\n",
      "--relu4_1 | shape=(1, 88, 66, 512) | bias_shape=(512,)\n",
      "--conv4_2 | shape=(1, 88, 66, 512) | weights_shape=(3, 3, 512, 512)\n",
      "--relu4_2 | shape=(1, 88, 66, 512) | bias_shape=(512,)\n",
      "--conv4_3 | shape=(1, 88, 66, 512) | weights_shape=(3, 3, 512, 512)\n",
      "--relu4_3 | shape=(1, 88, 66, 512) | bias_shape=(512,)\n",
      "--conv4_4 | shape=(1, 88, 66, 512) | weights_shape=(3, 3, 512, 512)\n",
      "--relu4_4 | shape=(1, 88, 66, 512) | bias_shape=(512,)\n",
      "--pool4   | shape=(1, 44, 33, 512)\n",
      "LAYER GROUP 5\n",
      "--conv5_1 | shape=(1, 44, 33, 512) | weights_shape=(3, 3, 512, 512)\n",
      "--relu5_1 | shape=(1, 44, 33, 512) | bias_shape=(512,)\n",
      "--conv5_2 | shape=(1, 44, 33, 512) | weights_shape=(3, 3, 512, 512)\n",
      "--relu5_2 | shape=(1, 44, 33, 512) | bias_shape=(512,)\n",
      "--conv5_3 | shape=(1, 44, 33, 512) | weights_shape=(3, 3, 512, 512)\n",
      "--relu5_3 | shape=(1, 44, 33, 512) | bias_shape=(512,)\n",
      "--conv5_4 | shape=(1, 44, 33, 512) | weights_shape=(3, 3, 512, 512)\n",
      "--relu5_4 | shape=(1, 44, 33, 512) | bias_shape=(512,)\n",
      "--pool5   | shape=(1, 22, 17, 512)\n",
      "\n",
      "MINIMIZING LOSS USING: ADAM OPTIMIZER\n",
      "At iterate 0\tf=  [4.8146684e+11]\n",
      "At iterate 20\tf=  [5.094587e+10]\n",
      "At iterate 40\tf=  [3.0874034e+10]\n",
      "At iterate 60\tf=  [2.4260583e+10]\n",
      "At iterate 80\tf=  [2.1170248e+10]\n",
      "At iterate 100\tf=  [1.9338504e+10]\n",
      "At iterate 120\tf=  [1.8105008e+10]\n",
      "At iterate 140\tf=  [1.7205707e+10]\n",
      "At iterate 160\tf=  [1.6521351e+10]\n",
      "At iterate 180\tf=  [1.5987104e+10]\n",
      "At iterate 200\tf=  [1.5556712e+10]\n",
      "At iterate 220\tf=  [1.5204111e+10]\n",
      "At iterate 240\tf=  [1.4911254e+10]\n",
      "At iterate 260\tf=  [1.4665499e+10]\n",
      "At iterate 280\tf=  [1.4585461e+10]\n",
      "At iterate 300\tf=  [1.4305976e+10]\n",
      "At iterate 320\tf=  [1.4141913e+10]\n",
      "At iterate 340\tf=  [1.4212323e+10]\n",
      "At iterate 360\tf=  [1.3910942e+10]\n",
      "At iterate 380\tf=  [1.3803088e+10]\n",
      "At iterate 400\tf=  [1.3788629e+10]\n",
      "At iterate 420\tf=  [1.365036e+10]\n",
      "At iterate 440\tf=  [1.3588093e+10]\n",
      "At iterate 460\tf=  [1.349661e+10]\n",
      "At iterate 480\tf=  [1.3896105e+10]\n",
      "Single image elapsed time: 346.33043098449707\n"
     ]
    }
   ],
   "source": [
    "render_single_image(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
